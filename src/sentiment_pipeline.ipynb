{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53144bd2",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d2156",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c7ecf",
   "metadata": {},
   "source": [
    "## 2. Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36497f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration (portable)\n",
    "BASE = Path(__file__).resolve().parents[1]\n",
    "RAW = BASE / \"data\" / \"raw\"\n",
    "# PROC: outputs directory (created if missing)\n",
    "PROC = BASE / \"data\" / \"processed\"\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input files\n",
    "TWEET_CSV = RAW / \"Tweet.csv\"\n",
    "MAP_CSV = RAW / \"Company_Tweet.csv\"\n",
    "\n",
    "# Output files generated by the pipeline\n",
    "OUT_TWEET = PROC / \"tweet_sentiment.csv\"\n",
    "OUT_TWEET_LABELED = PROC / \"tweet_sentiment_labeled.csv\"\n",
    "OUT_COMPANY = PROC / \"company_sentiment_daily.csv\"\n",
    "OUT_GLOBAL = PROC / \"global_sentiment_daily.csv\"\n",
    "OUT_TRAIN = PROC / \"tweet_sentiment_train_2015_2018.csv\"\n",
    "OUT_TEST = PROC / \"tweet_sentiment_test_2019_2020.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e18f7",
   "metadata": {},
   "source": [
    "## 3. Load & Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge():\n",
    "    # Load tweet dataset\n",
    "    tweets = pd.read_csv(TWEET_CSV, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    mapping = pd.read_csv(MAP_CSV)\n",
    "\n",
    "    # Remove rows without text or timestamp\n",
    "    tweets = tweets.dropna(subset=[\"body\", \"post_date\"])\n",
    "\n",
    "    # Merge ticker info\n",
    "    df = tweets.merge(mapping, on=\"tweet_id\", how=\"left\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76b06e",
   "metadata": {},
   "source": [
    "## 4. Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Normalize to lowercase\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs (http, https, www)\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)  # Keep only alphanumeric characters and spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single one\n",
    "    return text.strip()  # Strip leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e371277",
   "metadata": {},
   "source": [
    "## 5. Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406bca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"  # Use NVIDIA GPU if available\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"   # Use Apple Silicon GPU (M1/M2/M3/M4)\n",
    "    return \"cpu\"       # Default to CPU if no GPU backend is found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be73c54",
   "metadata": {},
   "source": [
    "## 6. Load FinBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finbert(device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")  # Load tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")  # Load model\n",
    "    model.to(device)  # Move model to selected device (CPU / CUDA / MPS)\n",
    "    model.eval()  # Set model to inference mode\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a27497",
   "metadata": {},
   "source": [
    "## 7. Sentiment Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(texts, tokenizer, model, device, batch_size=32):\n",
    "    results = []  # Store prediction dictionaries\n",
    "\n",
    "    # Process texts in batches for efficiency\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"FinBERT inference\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize and move to device\n",
    "        encoded = tokenizer(batch, padding=True, truncation=True,\n",
    "                            return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation (inference mode)\n",
    "            logits = model(**encoded).logits\n",
    "            scores = torch.softmax(logits, dim=1).cpu().numpy()  # Convert to probabilities\n",
    "\n",
    "        for s in scores:  # Append one dictionary per tweet\n",
    "            results.append({\n",
    "                \"positive\": float(s[0]),\n",
    "                \"neutral\": float(s[1]),\n",
    "                \"negative\": float(s[2])\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)  # Return as DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06d089",
   "metadata": {},
   "source": [
    "## 8. Daily Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29eea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df):\n",
    "    # Company-level daily sentiment (one row per ticker per date)\n",
    "    company = (\n",
    "        df.dropna(subset=[\"ticker_symbol\"])\n",
    "          .groupby([\"ticker_symbol\", \"date\"])[[\"positive\", \"neutral\", \"negative\"]]\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Global daily sentiment (one row per date)\n",
    "    global_ = (\n",
    "        df.groupby(\"date\")[[\"positive\", \"neutral\", \"negative\"]]\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    return company, global_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee8c80",
   "metadata": {},
   "source": [
    "## 9. Full Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    df = load_and_merge()  # Load Tweet.csv + Company_Tweet.csv\n",
    "\n",
    "    # Clean text\n",
    "    df[\"clean_text\"] = df[\"body\"].apply(clean_text)\n",
    "    df = df[df[\"clean_text\"] != \"\"]  # Remove empty cleaned tweets\n",
    "\n",
    "    # Convert Unix timestamp to date\n",
    "    df[\"date\"] = pd.to_datetime(df[\"post_date\"], unit=\"s\", utc=True,\n",
    "                                errors=\"coerce\").dt.date\n",
    "    df = df.dropna(subset=[\"date\"])  # Remove rows with invalid dates\n",
    "\n",
    "    # Keep only 2015â€“2020\n",
    "    df = df[(df[\"date\"] >= pd.to_datetime(\"2015-01-01\").date()) &\n",
    "            (df[\"date\"] <= pd.to_datetime(\"2020-12-31\").date())]\n",
    "\n",
    "    # Select device and load FinBERT\n",
    "    device = choose_device()\n",
    "    tokenizer, model = load_finbert(device)\n",
    "\n",
    "    # Sentiment prediction \n",
    "    preds = predict_sentiment(df[\"clean_text\"].tolist(),\n",
    "                              tokenizer, model, device)\n",
    "    preds = preds.reset_index(drop=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Row alignment check between tweets and predictions\n",
    "    assert len(df) == len(preds), \"Mismatch between tweets and predictions.\"\n",
    "\n",
    "    # Add sentiment results (FinBERT scores + labels)\n",
    "    df = pd.concat([df, preds], axis=1)\n",
    "    df[\"sentiment\"] = df[[\"positive\", \"neutral\", \"negative\"]].idxmax(axis=1)\n",
    "    df[\"polarity\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\": -1}).fillna(0)\n",
    "\n",
    "    # Save tweet-level outputs\n",
    "    df.to_csv(OUT_TWEET, index=False)\n",
    "    df.to_csv(OUT_TWEET_LABELED, index=False)\n",
    "\n",
    "    # Daily aggregations\n",
    "    company_daily, global_daily = aggregate(df)\n",
    "    company_daily.to_csv(OUT_COMPANY, index=False)\n",
    "    global_daily.to_csv(OUT_GLOBAL, index=False)\n",
    "\n",
    "    # Train-test split\n",
    "    train = df[(df[\"date\"] >= pd.to_datetime(\"2015-01-01\").date()) &\n",
    "               (df[\"date\"] <= pd.to_datetime(\"2018-12-31\").date())]\n",
    "\n",
    "    test = df[(df[\"date\"] >= pd.to_datetime(\"2019-01-01\").date()) &\n",
    "              (df[\"date\"] <= pd.to_datetime(\"2020-12-31\").date())]\n",
    "\n",
    "    train.to_csv(OUT_TRAIN, index=False)\n",
    "    test.to_csv(OUT_TEST, index=False)\n",
    "\n",
    "    print(\"\\nAll sentiment files generated successfully.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d159c",
   "metadata": {},
   "source": [
    "## 10. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
