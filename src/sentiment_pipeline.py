import pandas as pd
import numpy as np
import torch
import re
from tqdm import tqdm
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Path Configuration
# Get the base directory of the project (one level up from the script location)
BASE = Path(__file__).resolve().parents[1]
# Define paths for raw input data and processed output data
RAW = BASE / "data" / "raw"
PROC = BASE / "data" / "processed"
# Create the processed directory if it doesn't exist
PROC.mkdir(parents=True, exist_ok=True)

# Define input files
TWEET_CSV = RAW / "Tweet.csv"
MAP_CSV = RAW / "Company_Tweet.csv"

# Define output files generated by the sentiment pipeline
OUT_TWEET = PROC / "tweet_sentiment.csv"
OUT_COMPANY = PROC / "company_sentiment_daily.csv"

# Data Loading and Initial Merge
def load_and_merge():
    # Load tweet dataset, handling potential malformed lines
    tweets = pd.read_csv(TWEET_CSV, engine="python", on_bad_lines="skip")
    # Load mapping data (links tweet_id to ticker_symbol)
    mapping = pd.read_csv(MAP_CSV)

    # Remove tweets without essential information (body text or post timestamp)
    tweets = tweets.dropna(subset=["body", "post_date"])

    # Merge tweet data with ticker symbol mapping on the common 'tweet_id'
    # Use a left merge to keep all tweets, even those without a matching ticker
    df = tweets.merge(mapping, on="tweet_id", how="left")
    return df

# Text Preprocessing
def clean_text(text):
    # Convert text to string and lowercase
    text = str(text).lower()
    # Remove URLs (http, https, www)
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)
    # Remove mentions (@username) and hashtags (#topic)
    text = re.sub(r"@\w+|#\w+", "", text)
    # Keep only alphanumeric characters and spaces, replacing others with a space
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    # Replace multiple spaces with a single one
    text = re.sub(r"\s+", " ", text)
    # Remove leading/trailing whitespace
    return text.strip()

# Hardware Configuration
def choose_device():
    # Check for NVIDIA GPU (CUDA) availability
    if torch.cuda.is_available():
        return "cuda"
    # Check for Apple Silicon GPU (MPS) availability
    if torch.backends.mps.is_available():
        return "mps"
    # Default to CPU if no specialized device is found
    return "cpu"

# FinBERT Model Loading
def load_finbert(device):
    # Load the tokenizer for the FinBERT model
    tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
    # Load the pre-trained sequence classification model
    model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
    # Move the model to the selected device (GPU/CPU)
    model.to(device)
    # Set the model to evaluation mode (no training/gradient updates)
    model.eval()
    return tokenizer, model

# Sentiment Prediction
def predict_sentiment(texts, tokenizer, model, device, batch_size=32):
    results = [] # Initialize list to store prediction scores
    # Process texts in batches for improved inference efficiency using tqdm for progress tracking
    for i in tqdm(range(0, len(texts), batch_size), desc="FinBERT inference"):
        batch = texts[i:i+batch_size]
        # Tokenize the batch and move tensors to the selected device
        encoded = tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(device)

        # Disable gradient calculations (inference mode)
        with torch.no_grad():
            # Forward pass to get raw logits
            logits = model(**encoded).logits
            # Apply softmax to convert logits to probabilities, then move to CPU and convert to numpy
            scores = torch.softmax(logits, dim=1).cpu().numpy()

        # Store prediction scores (positive, neutral, negative)
        for s in scores:
            results.append({
                "positive": float(s[0]),
                "neutral": float(s[1]),
                "negative": float(s[2])
            })

    return pd.DataFrame(results)

# Company Daily Aggregation
def aggregate_company(df):
    # Calculate daily average sentiment scores per company (positive, neutral, negative)
    company_daily = (
        df.dropna(subset=["ticker_symbol"])
          .groupby(["ticker_symbol", "date"])[["positive", "neutral", "negative"]]
          .mean()
          .reset_index() # Convert groupby keys back into columns
    )
    return company_daily

# Main Execution Pipeline
def main():
    # Load raw data and merge with ticker mapping
    df = load_and_merge()

    # Apply text cleaning and filter out any resulting empty strings
    df["clean_text"] = df["body"].apply(clean_text)
    df = df[df["clean_text"] != ""]

    # Convert UNIX timestamp (seconds) to date objects and handle invalid dates
    df["date"] = pd.to_datetime(df["post_date"], unit="s", utc=True, errors="coerce").dt.date
    df = df.dropna(subset=["date"])

    # Filter data to include only the specified date range (2015-2020)
    df = df[(df["date"] >= pd.to_datetime("2015-01-01").date()) &
            (df["date"] <= pd.to_datetime("2020-12-31").date())]

    # Load hardware device and FinBERT model
    device = choose_device()
    tokenizer, model = load_finbert(device)

    # Predict sentiment scores for all cleaned tweets
    preds = predict_sentiment(df["clean_text"].tolist(), tokenizer, model, device)

    # Reset indices to ensure perfect alignment before concatenation
    preds = preds.reset_index(drop=True)
    df = df.reset_index(drop=True)

    # Safety check: ensure number of tweets matches number of predictions
    assert len(df) == len(preds)

    # Combine original data frame with sentiment scores
    df = pd.concat([df, preds], axis=1)

    # Determine final sentiment label (positive, neutral, or negative) based on the max score
    df["sentiment"] = df[["positive", "neutral", "negative"]].idxmax(axis=1)
    # Create numeric polarity column (1=positive, -1=negative, 0=neutral/other)
    df["polarity"] = df["sentiment"].map({"positive": 1, "negative": -1}).fillna(0)

    # Save the tweet-level sentiment data
    df.to_csv(OUT_TWEET, index=False)

    # Aggregate sentiment by company and save daily results
    company_daily = aggregate_company(df)
    company_daily.to_csv(OUT_COMPANY, index=False)
    
    # Print success messages
    print("\nFiles generated successfully:\n")
    print(f"- {OUT_TWEET.name}")
    print(f"- {OUT_COMPANY.name}")

if __name__ == "__main__":
    main()
